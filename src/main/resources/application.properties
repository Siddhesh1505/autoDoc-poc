spring.application.name=autodoc

# --- Spring AI & Ollama Configuration ---
spring.ai.ollama.base-url=http://localhost:11434
# Use llama3 for general docs or codellama for better code understanding
spring.ai.ollama.chat.options.model=llama3.1

# --- LLM Performance Settings ---
# Since code analysis is heavy, we increase the timeout
spring.ai.ollama.chat.options.num-predict=4096

# Increase context to 32k for Llama 3.1
spring.ai.ollama.chat.options.num-ctx=32768
# Keep temperature low for facts
spring.ai.ollama.chat.options.temperature=0.1

# --- Server Configuration ---
server.port=8080

# --- Optional: Git Auth (if using private repos) ---
# git.username=your_username
# git.token=your_pat_token